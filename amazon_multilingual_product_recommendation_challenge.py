# -*- coding: utf-8 -*-
"""Amazon_Multilingual_Product_Recommendation_Challenge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_610oBgts_YTQMMNWcH-7nBH_ctDiHcb
"""

!pip install aicrowd-cli pyarrow

!aicrowd login

!aicrowd dataset download --challenge task-1-next-product-recommendation

"""
# **Data Preprocessing**



"""

# from algorithms import Session
import numpy as np
import pandas as pd
from functools import lru_cache
import random
import re
import os
import torch
from gensim.models import Word2Vec

from google.colab import drive
drive.mount('/content/drive')

train_data_dir = '.'
test_data_dir = '.'
task = 'task1'
Task = 'task2'
PREDS_PER_SESSION = 100

# Cache loading of data for multiple calls

@lru_cache(maxsize=1)
def read_product_data():
    return pd.read_csv(os.path.join(train_data_dir, 'products_train.csv'))

@lru_cache(maxsize=1)
def read_train_data():
    return pd.read_csv(os.path.join(train_data_dir, 'sessions_train.csv'))

@lru_cache(maxsize=3)
def read_test_data(task):
    return pd.read_csv(os.path.join(test_data_dir, f'sessions_test_{task}.csv'))

def read_locale_data(locale, task):
    products = read_product_data().query(f'locale == "{locale}"')
    sess_train = read_train_data().query(f'locale == "{locale}"')
    sess_test = read_test_data(task).query(f'locale == "{locale}"')
    return products, sess_train, sess_test

def show_locale_info(locale, task):
    products, sess_train, sess_test = read_locale_data(locale, task)

    train_l = sess_train['prev_items'].apply(lambda sess: len(sess))
    test_l = sess_test['prev_items'].apply(lambda sess: len(sess))

    print(f"Locale: {locale} \n"
          f"Number of products: {products['id'].nunique()} \n"
          f"Number of train sessions: {len(sess_train)} \n"
          f"Train session lengths - "
          f"Mean: {train_l.mean():.2f} | Median {train_l.median():.2f} | "
          f"Min: {train_l.min():.2f} | Max {train_l.max():.2f} \n"
          f"Number of test sessions: {len(sess_test)}"
        )
    if len(sess_test) > 0:
        print(
             f"Test session lengths - "
            f"Mean: {test_l.mean():.2f} | Median {test_l.median():.2f} | "
            f"Min: {test_l.min():.2f} | Max {test_l.max():.2f} \n"
        )
    print("======================================================================== \n")

products = read_product_data()
train_sessions = read_train_data()

products.head()

train_sessions.head()

user_id=train_sessions.index
print(user_id)

train_sessions['user_id']=user_id

train_sessions

train_sessions_t1=train_sessions.copy()
train_sessions_t2=train_sessions.copy()
at=train_sessions[((train_sessions['locale'] == 'ES'))].index
bt=train_sessions[((train_sessions.locale=='IT'))].index
ct=train_sessions[((train_sessions.locale=='FR'))].index
dt=train_sessions[((train_sessions.locale=='DE'))].index
et=train_sessions[((train_sessions.locale=='JP'))].index
ft=train_sessions[((train_sessions.locale=='UK'))].index

at_rows = random.sample(list(at) ,89047)
bt_rows = random.sample(list(bt) ,126925)
ct_rows = random.sample(list(ct) ,117561)
dt_rows = random.sample(list(dt) ,1111416)
et_rows = random.sample(list(et) ,979119)
ft_rows = random.sample(list(ft) ,1182181)
train_sessions_t1 = train_sessions_t1.drop(at_rows)
train_sessions_t1 = train_sessions_t1.drop(bt_rows)
train_sessions_t1 = train_sessions_t1.drop(ct_rows)
train_sessions_t2 = train_sessions_t2.drop(dt_rows)
train_sessions_t2 = train_sessions_t2.drop(et_rows)
train_sessions_t2 = train_sessions_t2.drop(ft_rows)

train_sessions_DE=train_sessions_t1.drop(et_rows)
train_sessions_DE=train_sessions_DE.drop(ft_rows)
train_sessions_JP=train_sessions_t1.drop(dt_rows)
train_sessions_JP=train_sessions_JP.drop(ft_rows)
train_sessions_UK=train_sessions_t1.drop(dt_rows)
train_sessions_UK=train_sessions_UK.drop(et_rows)

train_sessions_ES=train_sessions_t2.drop(bt_rows)
train_sessions_ES=train_sessions_ES.drop(ct_rows)
train_sessions_IT=train_sessions_t2.drop(at_rows)
train_sessions_IT=train_sessions_IT.drop(ct_rows)
train_sessions_FR=train_sessions_t2.drop(at_rows)
train_sessions_FR=train_sessions_FR.drop(bt_rows)

products_t1=products.copy()
a=products_t1[((products_t1['locale'] == 'ES'))].index
b=products_t1[((products_t1.locale=='IT'))].index
c=products_t1[((products_t1.locale=='FR'))].index
d=products_t1[((products_t1.locale=='DE'))].index
e=products_t1[((products_t1.locale=='JP'))].index
f=products_t1[((products_t1.locale=='UK'))].index

products_t1.title.fillna('UNK' , inplace=True)
products_t1.brand.fillna('UNK' , inplace=True)
products_t1.color.fillna('UNK' , inplace=True)
#products_t1.size.fillna('UNK' , inplace=True)
products_t1.model.fillna('UNK' , inplace=True)
products_t1.material.fillna('UNK' , inplace=True)
products_t1.author.fillna('UNK' , inplace=True)
products_t1.desc.fillna('UNK' , inplace=True)

products_t1

import pandas as pd
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import re

!pip install simplemma

import simplemma

# Preprocess text data based on locale and specified columns
def preprocess_text(text, locale):
    if pd.isnull(text):
        return ''
    # Remove brackets, punctuation, and inverted commas
    text = re.sub(r"([.,!?])", r" \1 ", text)
    # text = re.sub(r"[^a-zA-Z]+", r" ", text)
    if text==' ':
        text='UNK'
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'[\'\"]', '', text)
    # text=text.split()

    # Tokenize the text
    tokens = word_tokenize(text.lower())

    # Define locale-specific stop words
    if locale == 'DE':
        stop_words = set(stopwords.words('german'))
        lang='de'
        # print(stop_words)
    elif locale == 'JP':
        stop_words = set(["あそこ","あっ","あの","あのかた","あの人","あり","あります","ある","あれ","い","いう","います","いる","う","うち","え","お","および","おり","おります","か","かつて","から","が","き","ここ","こちら","こと","この","これ","これら","さ","さらに","し","しかし","する","ず","せ","せる","そこ","そして","その","その他","その後","それ","それぞれ","それで","た","ただし","たち","ため","たり","だ","だっ","だれ","つ","て","で","でき","できる","です","では","でも","と","という","といった","とき","ところ","として","とともに","とも","と共に","どこ","どの","な","ない","なお","なかっ","ながら","なく","なっ","など","なに","なら","なり","なる","なん","に","において","における","について","にて","によって","により","による","に対して","に対する","に関する","の","ので","のみ","は","ば","へ","ほか","ほとんど","ほど","ます","また","または","まで","も","もの","ものの","や","よう","より","ら","られ","られる","れ","れる","を","ん","何","及び","彼","彼女","我々","特に","私","私達","貴方","貴方方"])
        lang ='en'
    elif locale == 'UK':
        stop_words = set(stopwords.words('english'))
        lang='en'
    elif locale == 'ES':
        stop_words = set(stopwords.words('spanish'))
        lang='es'
    elif locale == 'FR':
        stop_words = set(stopwords.words('french'))
        lang='fr'
    elif locale == 'IT':
        stop_words = set(stopwords.words('italian'))
        lang='it'
    else:
        stop_words = set()

    # Remove stop words and perform lemmatization
    # lemmatizer = simplemma.lemmatize(text, lang='en')
    tokens = [simplemma.lemmatize(token, lang) for token in tokens if token not in stop_words]

    # Join the tokens back into a single string
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text

import tensorflow as tf

# Initialize GPU
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('GPU device not found. Using CPU instead.')
    device_name = '/device:CPU:0'

# Assign the text preprocessing process to a GPU
with tf.device(device_name):
    # Define the columns to preprocess
    columns_to_preprocess = ['title', 'color', 'material', 'desc']

    # Preprocess the text columns based on the 'locale' column
    preprocessed_df = products_t1

    for column in columns_to_preprocess:
        preprocessed_df[column] = preprocessed_df.apply(lambda row: preprocess_text(row[column], row['locale']), axis=1)

    # Print the preprocessed dataset
    print(preprocessed_df)

# Saving the file
preprocessed_df.to_csv('preprocessed_df.csv', index=False)

# Downloading the file
from google.colab import files
files.download('preprocessed_df.csv')

from google.colab import drive
drive.mount('/content/drive')

!pip install -U -q PyDrive

import pandas as pd
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive

import pandas as pd
from google.colab import drive

products_t1 = pd.read_csv('preprocessed_df.csv')

products_t1

import re

def preprocess_text2(text):
    text = text.lower()
    text = re.sub(r"([.,!?])", r" \1 ", text)
    text = re.sub(r"\b[0-9]+\w*\b|\b[0-9]\b|\w*\d+\w*", "", text)
    text = re.sub(r"[^\w\s]", "", text)
    text = re.sub(r"[\'\"]", "", text)
    if text.strip() == "":
        text = "UNK"
    return text

def preprocess_text3(text):
    # text = text.lower()
    # text = re.sub(r"([.,!?])", r" \1 ", text)
    # text = re.sub(r"[^a-zA-Z]+", r" ", text)
    # text = re.sub(r'[^\w\s]', '', text)
    # text = re.sub(r'[\'\"]', '', text)
    if text == ' ':
        text = 'UNK'
    return text

import tensorflow as tf

# # Initialize GPU
# device_name = tf.test.gpu_device_name()
# if device_name != '/device:GPU:0':
#     print('GPU device not found. Using CPU instead.')
#     device_name = '/device:CPU:0'

# # Assign the text preprocessing process to a GPU
# with tf.device(device_name):
#     products_t1.title=products_t1.title.apply(preprocess_text2)
#     products_t1.brand=products_t1.brand.apply(preprocess_text2)
#     products_t1.color=products_t1.color.apply(preprocess_text2)
#     #products_t1.size=products_t1.size.apply(preprocess_text)
#     products_t1.model=products_t1.model.apply(preprocess_text2)
#     products_t1.material=products_t1.material.apply(preprocess_text2)
#     products_t1.author=products_t1.author.apply(preprocess_text2)
#     products_t1.desc=products_t1.desc.apply(preprocess_text2)

products_t1.title.fillna('UNK' , inplace=True)
products_t1.brand.fillna('UNK' , inplace=True)
products_t1.color.fillna('UNK' , inplace=True)
#products_t1.size.fillna('UNK' , inplace=True)
products_t1.model.fillna('UNK' , inplace=True)
products_t1.material.fillna('UNK' , inplace=True)
products_t1.author.fillna('UNK' , inplace=True)
products_t1.desc.fillna('UNK' , inplace=True)

# Initialize GPU
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('GPU device not found. Using CPU instead.')
    device_name = '/device:CPU:0'

# Assign the text preprocessing process to a GPU
with tf.device(device_name):
    products_t1.title=products_t1.title.apply(preprocess_text2)
    products_t1.brand=products_t1.brand.apply(preprocess_text2)
    products_t1.color=products_t1.color.apply(preprocess_text2)
    # products_t1.size=products_t1.size.apply(preprocess_text)
    products_t1.model=products_t1.model.apply(preprocess_text2)
    products_t1.material=products_t1.material.apply(preprocess_text2)
    products_t1.author=products_t1.author.apply(preprocess_text2)
    products_t1.desc=products_t1.desc.apply(preprocess_text2)

products_t1

products_t1 = pd.read_csv('products_t2.csv')

products_t1.title.fillna('UNK' , inplace=True)
products_t1.brand.fillna('UNK' , inplace=True)
products_t1.color.fillna('UNK' , inplace=True)
#products_t1.size.fillna('UNK' , inplace=True)
products_t1.model.fillna('UNK' , inplace=True)
products_t1.material.fillna('UNK' , inplace=True)
products_t1.author.fillna('UNK' , inplace=True)
products_t1.desc.fillna('UNK' , inplace=True)

products_t2 = products_t1.copy()

# Saving the file
products_t2.to_csv('products_t2.csv', index=False)

# Downloading the file
from google.colab import files
files.download('products_t2.csv')

a_rows = random.sample(list(a) ,42503)
b_rows = random.sample(list(b) ,50461)
c_rows = random.sample(list(c) ,44577)
d_rows = random.sample(list(d) ,518327)
e_rows = random.sample(list(e) ,395009)
f_rows = random.sample(list(f) ,500180)
products_t1 = products_t1.drop(a_rows)
products_t1 = products_t1.drop(b_rows)
products_t1 = products_t1.drop(c_rows)
products_t2 = products_t2.drop(d_rows)
products_t2 = products_t2.drop(e_rows)
products_t2 = products_t2.drop(f_rows)

products_DE=products_t1.drop(e_rows)
products_DE=products_DE.drop(f_rows)
products_JP=products_t1.drop(d_rows)
products_JP=products_JP.drop(f_rows)
products_UK=products_t1.drop(d_rows)
products_UK=products_UK.drop(e_rows)

products_ES=products_t2.drop(b_rows)
products_ES=products_ES.drop(c_rows)
products_IT=products_t2.drop(a_rows)
products_IT=products_IT.drop(c_rows)
products_FR=products_t2.drop(a_rows)
products_FR=products_FR.drop(b_rows)

products_DE=products_DE.reset_index()
products_JP=products_JP.reset_index()
products_UK=products_UK.reset_index()
products_ES=products_ES.reset_index()
products_IT=products_IT.reset_index()
products_FR=products_FR.reset_index()

products_ES.drop(labels='size',axis=1,inplace=True)

products_ES.isna().sum()

products_ES

test_task1_sesions=pd.read_csv(os.path.join(test_data_dir, f'sessions_test_task1.csv'))
test_task2_sesions=pd.read_csv(os.path.join(test_data_dir, f'sessions_test_task2.csv'))

user_task1_id=test_task1_sesions.index
user_task2_id=test_task2_sesions.index

test_task1_sesions['user_id']=user_task1_id
test_task2_sesions['user_id']=user_task2_id

locale_names = products['locale'].unique()
for locale in locale_names:
    show_locale_info(locale, Task)

A=test_task2_sesions[((test_task2_sesions.locale == 'ES'))].index
B=test_task2_sesions[((test_task2_sesions.locale=='IT'))].index
C=test_task2_sesions[((test_task2_sesions.locale=='FR'))].index
D=test_task1_sesions[((test_task1_sesions.locale=='DE'))].index
E=test_task1_sesions[((test_task1_sesions.locale=='JP'))].index
F=test_task1_sesions[((test_task1_sesions.locale=='UK'))].index

A_rows = random.sample(list(A) ,8177)
B_rows = random.sample(list(B) ,13992)
C_rows = random.sample(list(C) ,12521)
D_rows = random.sample(list(D) ,104568)
E_rows = random.sample(list(E) ,96467)
F_rows = random.sample(list(F) ,115937)

test_sessions_DE=test_task1_sesions.drop(E_rows)
test_sessions_DE=test_sessions_DE.drop(F_rows)
test_sessions_JP=test_task1_sesions.drop(D_rows)
test_sessions_JP=test_sessions_JP.drop(F_rows)
test_sessions_UK=test_task1_sesions.drop(D_rows)
test_sessions_UK=test_sessions_UK.drop(E_rows)
test_sessions_ES=test_task2_sesions.drop(B_rows)
test_sessions_ES=test_sessions_ES.drop(C_rows)
test_sessions_IT=test_task2_sesions.drop(A_rows)
test_sessions_IT=test_sessions_IT.drop(C_rows)
test_sessions_FR=test_task2_sesions.drop(A_rows)
test_sessions_FR=test_sessions_FR.drop(B_rows)

test_sessions_DE=test_sessions_DE.reset_index()
test_sessions_JP=test_sessions_JP.reset_index()
test_sessions_UK=test_sessions_UK.reset_index()
test_sessions_ES=test_sessions_ES.reset_index()
test_sessions_IT=test_sessions_IT.reset_index()
test_sessions_FR=test_sessions_FR.reset_index()

"""# **Working and Approaches through Profiling**"""

item_profiles = {}

# Iterate over each item in the item data
for index, row in products_ES.iterrows():
    item_id = row['id']

    # Extract relevant features from the item data
    profile = {
        'title': row['title'],
        'price': row['price'],
        'brand': row['brand'],
        'color': row['color'],
        'model': row['model'],
        'material': row['material'],
        'desc': row['desc'],
        # Add more features as per your requirements
    }

    # Store the item profile in the dictionary
    item_profiles[item_id] = profile

item_profiles

"""# **Train Sessions - Approaches**"""

# corpus = []

# for item_id in item_profiles:
#     for feature in item_profiles[item_id]:
#         feat = []
#         if feature != 'price':
#             value = item_profiles[item_id][feature]
#             if isinstance(value, str):  # Check if the value is a string
#                 for text in value.split():
#                     feat.append(text)
#             corpus.append(feat)

corpus=[]
for item_id in item_profiles:
  for feature in item_profiles[item_id]:
    feat=[]
    if feature!='price':
      for text in item_profiles[item_id][feature].split():
        feat.append(text)
      corpus.append(feat)

from gensim.models import Word2Vec
#corpus = [text for item_id in item_profiles for feature in item_profiles[item_id] if feature!='price' for text in item_profiles[item_id][feature].split()]
# Train a Word2Vec model on the multilingual corpus

model = Word2Vec(corpus, window=5, min_count=1, workers=4)

def encode_textual_features(textual_features):
  # Get the vector representation for each textual feature
  encoded_features = {}
  for item_id in item_profiles:
    features = {}
    for feature in item_profiles[item_id]:
      if feature!='price':
        encoded_feature=[]
        for word in item_profiles[item_id][feature].split():
      #print(word)
          if word in model.wv:
            encoded_feature.append(model.wv[word])
          else:
            print(feature,word)
        features[feature] = np.mean(encoded_feature, axis=0)
    encoded_features[item_id] = features
  return encoded_features

# Encode item profiles using Word2Vec
encoded_item_profiles = encode_textual_features(item_profiles)

"""# **A possible different approach using LSTMs**"""

import numpy as np
import torch
import torch.nn as nn

def encode_textual_features(textual_features, lstm_model):
    # Get the vector representation for each textual feature
    encoded_features = {}
    for item_id in item_profiles:
        features = {}
        for feature in item_profiles[item_id]:
            if feature != 'price':
                words = item_profiles[item_id][feature].split()
                encoded_feature = lstm_model.encode(words)
                features[feature] = encoded_feature
        encoded_features[item_id] = features
    return encoded_features

# Define an LSTM model class
# class LSTMModel(nn.Module):
#     def __init__(self, embedding_dim, hidden_dim):
#         super(LSTMModel, self).__init__()
#         self.embedding_dim = embedding_dim
#         self.hidden_dim = hidden_dim
#         self.embedding = nn.Embedding(vocab_size, embedding_dim)
#         self.lstm = nn.LSTM(embedding_dim, hidden_dim)

#     def forward(self, x):
#         embedded = self.embedding(x)
#         output, _ = self.lstm(embedded)
#         encoded_feature = torch.mean(output, dim=0)
#         return encoded_feature

# # Create an instance of the LSTM model
# embedding_dim = 100
# hidden_dim = 128
# vocab_size = len(model.wv.vocab)  # Assuming `model.wv` is the Word2Vec model
# lstm_model = LSTMModel(embedding_dim, hidden_dim)

# # Example usage
# encoded_textual_features = encode_textual_features(textual_features, lstm_model)

"""# **Continuation ...**"""

encoded_item_profiles

c=[]
for item_id in encoded_item_profiles:
  b=[]
  for feature in encoded_item_profiles[item_id]:
    a=encoded_item_profiles[item_id][feature]
    b.append(a)
  c.append(b)
encoded_item_profiles_np=np.array(c)

c

encoded_item_profiles_np

encoded_item_profiles_np.shape

"""# **Common Trains**"""

train_sessions_ES

def preprocess(text):
    # Remove brackets, punctuation, and inverted commas
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'[\'\"]', '', text)
    text=text.split()
    #text = f"'{text}'"
    # Reconstruct the processed text
    processed_text = text

    return processed_text

train_sessions_ES['prev_items']=train_sessions_ES['prev_items'].apply(preprocess)

train_sessions_ES

user_purchase_history = train_sessions_ES.groupby('user_id')['prev_items'].apply(list)

user_purchase_history=user_purchase_history.apply(lambda x: x[0] if isinstance(x, list) else x)

user_purchase_history

import tensorflow as tf

# Initialize GPU
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('GPU device not found. Using CPU instead.')
    device_name = '/device:CPU:0'

# Assign the text preprocessing process to a GPU
with tf.device(device_name):
    user_profiles={}
    for i in range(3272716, 3273716):
        purchases = user_purchase_history[i]
        user_features = ['price', 'title', 'brand', 'color', 'model', 'material', 'desc']
        profile = {feature: [] for feature in user_features}
        for purchase in purchases:
           for feature in user_features:
                profile[feature].append(products_ES.loc[products_ES['id'] == purchase, feature].tolist())
        user_profiles[i]=profile

def encode_textual_user_features_test(textual_features):
  # Get the vector representation for each textual feature
  encoded_user_feature={}
  for user_test_id in textual_features:
    encoded_features = {}
    for feature in textual_features[user_test_id]:
      if feature!='price':
        encoded_feature=[]
        for sentences in textual_features[user_test_id][feature]:
          for sentence in sentences:
            for word in sentence.split():
              if word in model.wv:
                encoded_feature.append(model.wv[word])
              else:
                print(feature,word)
        encoded_features[feature] = np.mean(encoded_feature, axis=0)
    encoded_user_feature[user_test_id]=encoded_features
  return encoded_user_feature

encoded_user_profile_train=encode_textual_user_features_test(user_profiles)

encoded_user_profile_train[3272716]

"""# **Approach 1**"""

c={}
for item_id in encoded_item_profiles:
  b=[]
  for feature in encoded_item_profiles[item_id]:
    if feature=='desc':
      a=encoded_item_profiles[item_id][feature]
      b.append(a)
  c[item_id]=b
encoded_item_profiles_dict=c

c={}
for user_id in encoded_user_profile_train:
  b=[]
  for feature in encoded_user_profile_train[user_id]:
    if feature=='desc':
      a=encoded_user_profile_train[user_id][feature]
      b.append(a)
  c[user_id]=b
encoded_user_profile_dict=c

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def recommend_items(user_id, top_n=100):
  user_profile = np.array(encoded_user_profile_dict[user_id])
  #print(user_profile)
  item_profiles = np.array(list(encoded_item_profiles_dict.values()))
  #print(item_profiles[0])
  #print(cosine_similarity(user_profile,item_profiles[0]))
  similarity=[]
  for submatrix in item_profiles:
    submatrix_np=(np.array(submatrix)).reshape(100,1)
    batch_result=np.dot(user_profile, submatrix.T)
    #print(batch_result)
    norm1=np.linalg.norm(user_profile)
    norm2=np.linalg.norm(submatrix)
    similarity_score=batch_result[0][0]/(norm1*norm2)
    #print(batch_result[0][0]/(norm1*norm2))
    #norm1[0]
    similarity.append(similarity_score)
    #print(cosine_similarity(user_profile, submatrix)[0][0])
    #similarity.append(cosine_similarity(user_profile, submatrix)[0][0])
  ranked_items = np.argsort(similarity)[::-1]
  print(similarity)
  # Select top-N items as recommendations
  recommended_array = ranked_items[:top_n]
  recommended_items=[]
  for x in recommended_array:
    recommended_items.append(products_ES['id'][x])
  #return recommended_items
  return similarity

# Example usage
user_id = 3272716
#recommended_items = recommend_items(user_id, top_n=100)
similar=recommend_items(user_id, top_n=100)
#print("Recommended items for user", user_id, ":", recommended_items)
print(1)

print("Recommended items for user", user_id, ":", similar)

"""# **Approach 2**"""

def encode_textual_user_features(textual_features):
  # Get the vector representation for each textual feature
  encoded_features = {}
  for feature in textual_features:
    if feature!='price':
      encoded_feature=[]
      for sentences in textual_features[feature]:
        for sentence in sentences:
          for word in sentence.split():
            if word in model.wv:
              encoded_feature.append(model.wv[word])
            else:
              print(feature,word)
      encoded_features[feature] = np.mean(encoded_feature, axis=0)
  return encoded_features

purchases = user_purchase_history[3272720]

# Collect the user profile features from the purchases
user_features = ['price', 'title', 'brand', 'color', 'model', 'material', 'desc']
profile = {feature: [] for feature in user_features}

for purchase in purchases:
  for feature in user_features:
    profile[feature].append(products_ES.loc[products_ES['id'] == purchase, feature].tolist())

encoded_user_profile=encode_textual_user_features(profile)

encoded_user_profile

from sklearn.metrics.pairwise import cosine_similarity
def recommend_items(user_id, item_profiles, top_n=100):
  purchases = user_purchase_history[user_id]

  # Collect the user profile features from the purchases
  user_features = ['price', 'title', 'brand', 'color', 'model', 'material', 'desc']
  profile = {feature: [] for feature in user_features}

  for purchase in purchases:
    for feature in user_features:
      profile[feature].append(products_ES.loc[products_ES['id'] == purchase, feature].tolist())

  encoded_user_profile=encode_textual_user_features(profile)
  # Calculate the similarity between user profile and item profiles
  similarities = {}
  for item_id in encoded_item_profiles:
    for feature in encoded_item_profiles[item_id]:
      similarity = 0.0
      item_feature_values = encoded_item_profiles[item_id][feature]
      user_feature_values = encoded_user_profile[feature]
      feature_similarity = cosine_similarity([user_feature_values], [item_feature_values])

      if item_id=='B09G6JM5WM':
        if feature=='desc':
          print([user_feature_values])
          print([item_feature_values])
          print(cosine_similarity([user_feature_values], [item_feature_values]))
        #print(feature_similarity)
      similarity = similarity + feature_similarity
      #if item_id=='B09G6JM5WM':
        #print(similarity)

      similarities[item_id] = similarity

  # Rank items based on similarity
  #print(similarities)
  ranked_items = sorted(similarities, key=similarities.get, reverse=True)

  # Select top-N items as recommendations
  recommended_items = [item_id for item_id in ranked_items[:top_n]]

  return recommended_items

# Initialize GPU
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('GPU device not found. Using CPU instead.')
    device_name = '/device:CPU:0'

# Assign the text preprocessing process to a GPU
with tf.device(device_name):
  # Example usage
  user_id = 3272720
  recommended_items = recommend_items(user_id, item_profiles, top_n=100)
  #print("Recommended items for user", user_id, ":", recommended_items)
  print(1)

print("Recommended items for user", user_id, ":", recommended_items)

"""# **Approach 3**"""

c=[]
for item_id in encoded_item_profiles:
  b=[]
  for feature in encoded_item_profiles[item_id]:
    if feature=='desc':
      a=encoded_item_profiles[item_id][feature]
      b.append(a)
  c.append(b)
encoded_item_profiles_np=np.array(c)

c=[]
for user_id in encoded_user_profile_train:
  b=[]
  for feature in encoded_user_profile_train[user_id]:
    if feature=='desc':
      a=encoded_user_profile_train[user_id][feature]
      b.append(a)
  c.append(b)
encoded_user_profile_train_np=np.array(c)

matrix1=encoded_item_profiles_np.reshape(42503,100,1)
matrix2=encoded_user_profile_train_np
print(matrix1.shape,matrix2.shape)

batch_size = 500  # Adjust the batch size as per your memory constraints
# Split matrix1 and matrix2 into batches
matrix1_batches = np.array_split(matrix1, len(matrix1) // batch_size)
matrix2_batches = np.array_split(matrix2, len(matrix2) // batch_size)

if len(matrix1) % batch_size != 0:
  rem1_len=len(matrix1) % batch_size
  rem1=(matrix1[-rem1_len:])#.reshape(1,rem1_len,100,1)
  comp1=matrix1[:-rem1_len]
  print(comp1.shape)
else:
  comp1=matrix1
if len(matrix2) % batch_size != 0:
  rem2_len=len(matrix2) % batch_size
  rem2=(matrix2[-rem2_len:])#.reshape(1,rem2_len,1,100)
  comp2=matrix2[:-rem2_len]
  print(comp2.shape)
else:
  comp2=matrix2

batch_size = 500  # Adjust the batch size as per your memory constraints
# Split matrix1 and matrix2 into batches
matrix1_batches = np.array_split(comp1, len(comp1) // batch_size)
matrix2_batches = np.array_split(comp2, len(comp2) // batch_size)
print(np.array(matrix1_batches).shape)

mat1_batches=[]
if len(matrix1) % batch_size != 0:
  for x in matrix1_batches:
    mat1_batches.append(x)
  mat1_batches.append(rem1)
else:
  mat1_batches=matrix1_batches

mat2_batches=[]
if len(matrix2) % batch_size != 0:
  for x in matrix2_batches:
    mat2_batches.append(x)
  mat2_batches.append(rem2)
else:
  mat2_batches=matrix2_batches

with tf.device(device_name):
    result_train = np.zeros((1000, 42503))
    from sklearn.metrics.pairwise import cosine_similarity
    for i, batch2 in enumerate(mat2_batches):
      print(batch2.shape)
      p=-1+batch_size*i
      for submatrix2 in batch2:
        p=p+1
        #print(batch1.shape)
        # Perform dot product on the current batch pair
        for j, batch1 in enumerate(mat1_batches):
          #print(batch1.shape)
          q=-1+batch_size*j
          for submatrix1 in batch1:
            q=q+1
            batch_result=np.dot(submatrix2,submatrix1)
            norm_sub1=np.linalg.norm(submatrix1)
            norm_sub2=np.linalg.norm(submatrix2)
            similarity_value=batch_result[0][0]/(norm_sub1*norm_sub2)
            #print(similarity_value)
            #norm_sub1[0]
            result_train[p][q] = similarity_value
    print(j)

for i in range(1010):
  if result_train[0][i]==similar[i]:
    continue
  else:
    print(i)

def recommend_items(user_id, top_n=100):
  a=user_id - 3272716
  similarity=result_train[a]
  ranked_items = np.argsort(similarity)[::-1]
  recommended_array = ranked_items[:top_n]
  recommended_items=[]
  for x in recommended_array:
    recommended_items.append(products_ES['id'][x])
  return recommended_items

predicted_item=pd.DataFrame(columns=['locale','next_item_purchased'], index=range(1000))
for user_id in range(3272716, 3273716):
  recommended_items = recommend_items(user_id, top_n=100)
  a=user_id - 3272716
  predicted_item['locale'][a]='ES'
  predicted_item['next_item_purchased'][a]=recommended_items

predicted_item

with tf.device(device_name):
    MR=0
    n=0
    for i in range(3272716, 3273716):
      n=n+1
      user_id=i
      p=i-3272716
      x=train_sessions_ES['next_item'][i]
      recommended_items=predicted_item['next_item_purchased'][p]
      a=1
      for j in recommended_items:
          if x==j:
              break
          else:
              a+=1
      if a<101:
          pred_score=1/a
      else:
          pred_score=0
      MR=(MR*(n-1)+pred_score)/n
      print(MR)

"""# **Test_ES : Espagnol**"""

test_sessions_ES

test_sessions_ES['prev_items']=test_sessions_ES['prev_items'].apply(preprocess)

test_sessions_ES

user_purchase_history_test = test_sessions_ES.groupby('user_id')['prev_items'].apply(list)

user_purchase_history_test=user_purchase_history_test.apply(lambda x: x[0] if isinstance(x, list) else x)

user_purchase_history_test

with tf.device(device_name):
  user_profiles_ES={}
  for i in range(0, 8177):
    purchases = user_purchase_history_test[i]
    user_features = ['price', 'title', 'brand', 'color', 'model', 'material', 'desc']
    profile = {feature: [] for feature in user_features}
    for purchase in purchases:
      for feature in user_features:
        profile[feature].append(products_ES.loc[products_ES['id'] == purchase, feature].tolist())
    user_profiles_ES[i]=profile

user_profiles_ES

def encode_textual_user_features_test(textual_features):
  # Get the vector representation for each textual feature
  encoded_user_feature={}
  for user_test_id in textual_features:
    encoded_features = {}
    for feature in textual_features[user_test_id]:
      if feature!='price':
        encoded_feature=[]
        for sentences in textual_features[user_test_id][feature]:
          for sentence in sentences:
            for word in sentence.split():
              if word in model.wv:
                encoded_feature.append(model.wv[word])
              else:
                print(feature,word)
        encoded_features[feature] = np.mean(encoded_feature, axis=0)
    encoded_user_feature[user_test_id]=encoded_features
  return encoded_user_feature

encoded_user_profile_ES=encode_textual_user_features_test(user_profiles_ES)

c=[]
for item_id in encoded_item_profiles:
  b=[]
  for feature in encoded_item_profiles[item_id]:
    if feature=='desc':
      a=encoded_item_profiles[item_id][feature]
      b.append(a)
  c.append(b)
encoded_item_profiles_np=np.array(c)

c=[]
for user_id in encoded_user_profile_ES:
  b=[]
  for feature in encoded_user_profile_ES[user_id]:
    if feature=='desc':
      a=encoded_user_profile_ES[user_id][feature]
      b.append(a)
  c.append(b)
encoded_user_profile_ES_np=np.array(c)

matrix1=encoded_item_profiles_np.reshape(42503,100,1)
matrix2=encoded_user_profile_ES_np
print(matrix1.shape,matrix2.shape)

batch_size = 500
if len(matrix1) % batch_size != 0:
  rem1_len=len(matrix1) % batch_size
  rem1=(matrix1[-rem1_len:])#.reshape(1,rem1_len,100,1)
  comp1=matrix1[:-rem1_len]
  print(comp1.shape)
else:
  comp1=matrix1
if len(matrix2) % batch_size != 0:
  rem2_len=len(matrix2) % batch_size
  rem2=(matrix2[-rem2_len:])#.reshape(1,rem2_len,1,100)
  comp2=matrix2[:-rem2_len]
  print(comp2.shape)
else:
  comp2=matrix2

# Split matrix1 and matrix2 into batches
matrix1_batches = np.array_split(comp1, len(comp1) // batch_size)
matrix2_batches = np.array_split(comp2, len(comp2) // batch_size)
print(np.array(matrix1_batches).shape)

mat1_batches=[]
if len(matrix1) % batch_size != 0:
  for x in matrix1_batches:
    mat1_batches.append(x)
  mat1_batches.append(rem1)
else:
  mat1_batches=matrix1_batches

mat2_batches=[]
if len(matrix2) % batch_size != 0:
  for x in matrix2_batches:
    mat2_batches.append(x)
  mat2_batches.append(rem2)
else:
  mat2_batches=matrix2_batches

import tensorflow as tf

# Initialize GPU
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('GPU device not found. Using CPU instead.')
    device_name = '/device:CPU:0'

# Assign the text preprocessing process to a GPU
with tf.device(device_name):
  result_train = np.zeros((8177, 42503))
  from sklearn.metrics.pairwise import cosine_similarity
  for i, batch2 in enumerate(mat2_batches):
    print(batch2.shape)
    p=-1+batch_size*i
    for submatrix2 in batch2:
      p=p+1
      #print(batch1.shape)
      # Perform dot product on the current batch pair
      for j, batch1 in enumerate(mat1_batches):
          #print(batch1.shape)
          q=-1+batch_size*j
          for submatrix1 in batch1:
              q=q+1
              batch_result=np.dot(submatrix2,submatrix1)
              norm_sub1=np.linalg.norm(submatrix1)
              norm_sub2=np.linalg.norm(submatrix2)
              similarity_value=batch_result[0][0]/(norm_sub1*norm_sub2)
              #print(similarity_value)
              #norm_sub1[0]
              result_train[p][q] = similarity_value

def recommend_items_test(user_id, top_n=100):
  similarity=result_train[user_id]
  ranked_items = np.argsort(similarity)[::-1]
  recommended_array = ranked_items[:top_n]
  recommended_items=[]
  for x in recommended_array:
    recommended_items.append(products_ES['id'][x])
  return recommended_items

# Initialize GPU
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('GPU device not found. Using CPU instead.')
    device_name = '/device:CPU:0'

# Assign the text preprocessing process to a GPU
with tf.device(device_name):
  predicted_item_task2=pd.DataFrame(columns=['locale','next_item_purchased'], index=range(34690))
  for user_id in range(8177):
    recommended_items = recommend_items_test(user_id, top_n=100)
    predicted_item_task2['locale'][user_id]='ES'
    predicted_item_task2['next_item_purchased'][user_id]=recommended_items

predicted_item_task2.head(8180)

predicted_item_task2.to_csv('submission_task2.csv', index=False)

from google.colab import files
files.download('submission_task2.csv')

"""# **Test_FR : French**"""

item_profiles_FR = {}

# Iterate over each item in the item data
for index, row in products_FR.iterrows():
    item_id = row['id']

    # Extract relevant features from the item data
    profile = {
        'title': row['title'],
        'price': row['price'],
        'brand': row['brand'],
        'color': row['color'],
        'model': row['model'],
        'material': row['material'],
        'desc': row['desc'],
        # Add more features as per your requirements
    }

    # Store the item profile in the dictionary
    item_profiles_FR[item_id] = profile

item_profiles_FR

corpus=[]
for item_id in item_profiles_FR:
  for feature in item_profiles_FR[item_id]:
    feat=[]
    if feature!='price':
      for text in item_profiles_FR[item_id][feature].split():
        feat.append(text)
      corpus.append(feat)

model_FR = Word2Vec(corpus, window=5, min_count=1, workers=4)

def encode_textual_features_FR(textual_features):
  # Get the vector representation for each textual feature
  encoded_features = {}
  for item_id in item_profiles_FR:
    features = {}
    for feature in item_profiles_FR[item_id]:
      if feature!='price':
        encoded_feature=[]
        for word in item_profiles_FR[item_id][feature].split():
      #print(word)
          if word in model_FR.wv:
            encoded_feature.append(model_FR.wv[word])
          else:
            print(feature,word)
        features[feature] = np.mean(encoded_feature, axis=0)
    encoded_features[item_id] = features
  return encoded_features

encoded_item_profiles_FR = encode_textual_features_FR(item_profiles_FR)

c=[]
for item_id in encoded_item_profiles_FR:
  b=[]
  for feature in encoded_item_profiles_FR[item_id]:
    if feature=='desc':
      a=encoded_item_profiles_FR[item_id][feature]
      b.append(a)
  c.append(b)
encoded_item_profiles_np_FR=np.array(c)

encoded_item_profiles_np_FR.shape

test_sessions_FR

def preprocess(text):
    # Remove brackets, punctuation, and inverted commas
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'[\'\"]', '', text)
    text=text.split()
    #text = f"'{text}'"
    # Reconstruct the processed text
    processed_text = text

    return processed_text

test_sessions_FR['prev_items']=test_sessions_FR['prev_items'].apply(preprocess)

test_sessions_FR

user_purchase_history_FR = test_sessions_FR.groupby('user_id')['prev_items'].apply(list)

user_purchase_history_FR=user_purchase_history_FR.apply(lambda x: x[0] if isinstance(x, list) else x)

user_purchase_history_FR

import tensorflow as tf

# Initialize GPU
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('GPU device not found. Using CPU instead.')
    device_name = '/device:CPU:0'

with tf.device(device_name):
  user_profiles_FR={}
  for i in range(8177, 20698):
    purchases = user_purchase_history_FR[i]
    user_features = ['price', 'title', 'brand', 'color', 'model', 'material', 'desc']
    profile = {feature: [] for feature in user_features}
    for purchase in purchases:
      for feature in user_features:
        profile[feature].append(products_FR.loc[products_FR['id'] == purchase, feature].tolist())
    user_profiles_FR[i]=profile

user_profiles_FR

def encode_textual_user_features_test_FR(textual_features):
  # Get the vector representation for each textual feature
  encoded_user_feature={}
  for user_test_id in textual_features:
    encoded_features = {}
    for feature in textual_features[user_test_id]:
      if feature!='price':
        encoded_feature=[]
        for sentences in textual_features[user_test_id][feature]:
          for sentence in sentences:
            for word in sentence.split():
              if word in model_FR.wv:
                encoded_feature.append(model_FR.wv[word])
              else:
                print(feature,word)
        encoded_features[feature] = np.mean(encoded_feature, axis=0)
    encoded_user_feature[user_test_id]=encoded_features
  return encoded_user_feature

encoded_user_profile_FR=encode_textual_user_features_test_FR(user_profiles_FR)

c=[]
for user_id in encoded_user_profile_FR:
  b=[]
  for feature in encoded_user_profile_FR[user_id]:
    if feature=='desc':
      a=encoded_user_profile_FR[user_id][feature]
      b.append(a)
  c.append(b)
encoded_user_profile_FR_np=np.array(c)

matrix1=encoded_item_profiles_np_FR.reshape(44577,100,1)
matrix2=encoded_user_profile_FR_np
print(matrix1.shape,matrix2.shape)

batch_size = 500
if len(matrix1) % batch_size != 0:
  rem1_len=len(matrix1) % batch_size
  rem1=(matrix1[-rem1_len:])#.reshape(1,rem1_len,100,1)
  comp1=matrix1[:-rem1_len]
  print(comp1.shape)
else:
  comp1=matrix1
if len(matrix2) % batch_size != 0:
  rem2_len=len(matrix2) % batch_size
  rem2=(matrix2[-rem2_len:])#.reshape(1,rem2_len,1,100)
  comp2=matrix2[:-rem2_len]
  print(comp2.shape)
else:
  comp2=matrix2

# Split matrix1 and matrix2 into batches
matrix1_batches = np.array_split(comp1, len(comp1) // batch_size)
matrix2_batches = np.array_split(comp2, len(comp2) // batch_size)
print(np.array(matrix1_batches).shape)

mat1_batches=[]
if len(matrix1) % batch_size != 0:
  for x in matrix1_batches:
    mat1_batches.append(x)
  mat1_batches.append(rem1)
else:
  mat1_batches=matrix1_batches

mat2_batches=[]
if len(matrix2) % batch_size != 0:
  for x in matrix2_batches:
    mat2_batches.append(x)
  mat2_batches.append(rem2)
else:
  mat2_batches=matrix2_batches

# Initialize GPU
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('GPU device not found. Using CPU instead.')
    device_name = '/device:CPU:0'

# Assign the text preprocessing process to a GPU
with tf.device(device_name):
  result_FR = np.zeros((12521, 44577))
  from sklearn.metrics.pairwise import cosine_similarity
  for i, batch2 in enumerate(mat2_batches):
    print(batch2.shape)
    p=-1+batch_size*i
    for submatrix2 in batch2:
      p=p+1
      #print(batch1.shape)
      # Perform dot product on the current batch pair
      for j, batch1 in enumerate(mat1_batches):
          #print(batch1.shape)
          q=-1+batch_size*j
          for submatrix1 in batch1:
              q=q+1
              batch_result=np.dot(submatrix2,submatrix1)
              norm_sub1=np.linalg.norm(submatrix1)
              norm_sub2=np.linalg.norm(submatrix2)
              similarity_value=batch_result[0][0]/(norm_sub1*norm_sub2)
              #print(similarity_value)
              #norm_sub1[0]
              result_FR[p][q] = similarity_value

def recommend_items_test(user_id, top_n=100):
  similarity=result_FR[user_id]
  ranked_items = np.argsort(similarity)[::-1]
  recommended_array = ranked_items[:top_n]
  recommended_items=[]
  for x in recommended_array:
    recommended_items.append(products_FR['id'][x])
  return recommended_items

predicted_item_task2=pd.read_csv(r"/content/submission_task2.csv")

predicted_item_task2

for user_id in range(8177, 20698):
  a=user_id-8177
  recommended_items = recommend_items_test(a, top_n=100)
  predicted_item_task2['locale'][user_id]='FR'
  predicted_item_task2['next_item_purchased'][user_id]=recommended_items

predicted_item_task2.head(20700)

predicted_item_task2.to_csv('submission_task2.csv', index=False)

from google.colab import files
files.download('submission_task2.csv')

"""# **Test_IT : Italian**

"""

item_profiles_IT = {}

# Iterate over each item in the item data
for index, row in products_IT.iterrows():
    item_id = row['id']

    # Extract relevant features from the item data
    profile = {
        'title': row['title'],
        'price': row['price'],
        'brand': row['brand'],
        'color': row['color'],
        'model': row['model'],
        'material': row['material'],
        'desc': row['desc'],
        # Add more features as per your requirements
    }

    # Store the item profile in the dictionary
    item_profiles_IT[item_id] = profile

item_profiles_IT

corpus=[]
for item_id in item_profiles_IT:
  for feature in item_profiles_IT[item_id]:
    feat=[]
    if feature!='price':
      for text in item_profiles_IT[item_id][feature].split():
        feat.append(text)
      corpus.append(feat)

model_IT = Word2Vec(corpus, window=5, min_count=1, workers=4)

def encode_textual_features_IT(textual_features):
  # Get the vector representation for each textual feature
  encoded_features = {}
  for item_id in item_profiles_IT:
    features = {}
    for feature in item_profiles_IT[item_id]:
      if feature!='price':
        encoded_feature=[]
        for word in item_profiles_IT[item_id][feature].split():
      #print(word)
          if word in model_IT.wv:
            encoded_feature.append(model_IT.wv[word])
          else:
            print(feature,word)
        features[feature] = np.mean(encoded_feature, axis=0)
    encoded_features[item_id] = features
  return encoded_features

encoded_item_profiles_IT = encode_textual_features_IT(item_profiles_IT)

c=[]
for item_id in encoded_item_profiles_IT:
  b=[]
  for feature in encoded_item_profiles_IT[item_id]:
    if feature=='desc':
      a=encoded_item_profiles_IT[item_id][feature]
      b.append(a)
  c.append(b)
encoded_item_profiles_np_IT=np.array(c)

encoded_item_profiles_np_IT.shape

test_sessions_IT

def preprocess(text):
    # Remove brackets, punctuation, and inverted commas
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'[\'\"]', '', text)
    text=text.split()
    #text = f"'{text}'"
    # Reconstruct the processed text
    processed_text = text

    return processed_text

test_sessions_IT['prev_items']=test_sessions_IT['prev_items'].apply(preprocess)

test_sessions_IT

user_purchase_history_IT = test_sessions_IT.groupby('user_id')['prev_items'].apply(list)

user_purchase_history_IT=user_purchase_history_IT.apply(lambda x: x[0] if isinstance(x, list) else x)

user_purchase_history_IT

import tensorflow as tf

# Initialize GPU
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('GPU device not found. Using CPU instead.')
    device_name = '/device:CPU:0'

with tf.device(device_name):
  user_profiles_IT={}
  for i in range(20698, 34690):
    purchases = user_purchase_history_IT[i]
    user_features = ['price', 'title', 'brand', 'color', 'model', 'material', 'desc']
    profile = {feature: [] for feature in user_features}
    for purchase in purchases:
      for feature in user_features:
        profile[feature].append(products_IT.loc[products_IT['id'] == purchase, feature].tolist())
    user_profiles_IT[i]=profile

user_profiles_IT

# Saving the file
user_profiles_IT.to_csv('user_profiles_IT.csv', index=False)

# Downloading the file
from google.colab import files
files.download('user_profiles_IT.csv')

def encode_textual_user_features_test_IT(textual_features):
  # Get the vector representation for each textual feature
  encoded_user_feature={}
  for user_test_id in textual_features:
    encoded_features = {}
    for feature in textual_features[user_test_id]:
      if feature!='price':
        encoded_feature=[]
        for sentences in textual_features[user_test_id][feature]:
          for sentence in sentences:
            for word in sentence.split():
              if word in model_IT.wv:
                encoded_feature.append(model_IT.wv[word])
              else:
                print(feature,word)
        encoded_features[feature] = np.mean(encoded_feature, axis=0)
    encoded_user_feature[user_test_id]=encoded_features
  return encoded_user_feature

encoded_user_profile_IT=encode_textual_user_features_test_IT(user_profiles_IT)

c=[]
for user_id in encoded_user_profile_IT:
  b=[]
  for feature in encoded_user_profile_IT[user_id]:
    if feature=='desc':
      a=encoded_user_profile_IT[user_id][feature]
      b.append(a)
  c.append(b)
encoded_user_profile_IT_np=np.array(c)

matrix1=encoded_item_profiles_np_IT.reshape(50461,100,1)
matrix2=encoded_user_profile_IT_np
print(matrix1.shape,matrix2.shape)

batch_size = 500
if len(matrix1) % batch_size != 0:
  rem1_len=len(matrix1) % batch_size
  rem1=(matrix1[-rem1_len:])#.reshape(1,rem1_len,100,1)
  comp1=matrix1[:-rem1_len]
  print(comp1.shape)
else:
  comp1=matrix1
if len(matrix2) % batch_size != 0:
  rem2_len=len(matrix2) % batch_size
  rem2=(matrix2[-rem2_len:])#.reshape(1,rem2_len,1,100)
  comp2=matrix2[:-rem2_len]
  print(comp2.shape)
else:
  comp2=matrix2

# Split matrix1 and matrix2 into batches
matrix1_batches = np.array_split(comp1, len(comp1) // batch_size)
matrix2_batches = np.array_split(comp2, len(comp2) // batch_size)
print(np.array(matrix1_batches).shape)

mat1_batches=[]
if len(matrix1) % batch_size != 0:
  for x in matrix1_batches:
    mat1_batches.append(x)
  mat1_batches.append(rem1)
else:
  mat1_batches=matrix1_batches

mat2_batches=[]
if len(matrix2) % batch_size != 0:
  for x in matrix2_batches:
    mat2_batches.append(x)
  mat2_batches.append(rem2)
else:
  mat2_batches=matrix2_batches

# Initialize GPU
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('GPU device not found. Using CPU instead.')
    device_name = '/device:CPU:0'

# Assign the text preprocessing process to a GPU
with tf.device(device_name):
  result_IT = np.zeros((13992, 50461))
  from sklearn.metrics.pairwise import cosine_similarity
  for i, batch2 in enumerate(mat2_batches):
    print(batch2.shape)
    p=-1+batch_size*i
    for submatrix2 in batch2:
      p=p+1
      #print(batch1.shape)
      # Perform dot product on the current batch pair
      for j, batch1 in enumerate(mat1_batches):
          #print(batch1.shape)
          q=-1+batch_size*j
          for submatrix1 in batch1:
              q=q+1
              batch_result=np.dot(submatrix2,submatrix1)
              norm_sub1=np.linalg.norm(submatrix1)
              norm_sub2=np.linalg.norm(submatrix2)
              similarity_value=batch_result[0][0]/(norm_sub1*norm_sub2)
              #print(similarity_value)
              #norm_sub1[0]
              result_IT[p][q] = similarity_value

def recommend_items_test(user_id, top_n=100):
  similarity=result_IT[user_id]
  ranked_items = np.argsort(similarity)[::-1]
  recommended_array = ranked_items[:top_n]
  recommended_items=[]
  for x in recommended_array:
    recommended_items.append(products_IT['id'][x])
  return recommended_items

predicted_item_task2=pd.read_csv('submission_task.csv')

# Set the desired index range
start_index = 20700
end_index = 34691

# Calculate the number of rows to be added
num_rows = end_index - start_index + 1

# Create a DataFrame with NaN values
nan_values = pd.DataFrame(np.nan, index=range(start_index, end_index + 1), columns=predicted_item_task2.columns)

# Concatenate the original DataFrame and the NaN DataFrame
predicted_item_task2 = pd.concat([predicted_item_task2, nan_values])

# Reset the index of the DataFrame
predicted_item_task2 = predicted_item_task2.reset_index(drop=True)

for user_id in range(20698, 34690):
    a=user_id-20698
    recommended_items = recommend_items_test(a, top_n=100)
    predicted_item_task2['locale'][user_id]='IT'
    predicted_item_task2['next_item_purchased'][user_id]=recommended_items

predicted_item_task2.columns = ['locale', 'next_item_prediction']

predicted_item_task2

predicted_item_task2.to_csv('submission_task2.csv', index=False)

from google.colab import files
files.download('submission_task2.csv')

from IPython.display import FileLink

# Specify the file path of the created file
file_path = "submission_task2.csv"

# Create a FileLink object with the file path
file_link = FileLink(file_path)

# Display the download link
display(file_link)

"""# **AICrowd Submission**"""

predicted_item_task2 = pd.read_csv('/content/drive/MyDrive/VLGproj/submission_task2.csv')

predicted_item_task2.to_parquet(f'submission_{Task}.parquet', engine='pyarrow')

import pandas as pd
import pyarrow as pa

# Convert the problematic column to a string representation
predicted_item_task2['next_item_prediction'] = predicted_item_task2['next_item_prediction'].astype(str)

if 'next_item_prediction' in predicted_item_task2.columns:
    # Convert the DataFrame to a PyArrow Table
    table = pa.Table.from_pandas(predicted_item_task2)

    # Save the PyArrow Table as a Parquet file
    pa.parquet.write_table(table, 'submission_task2.parquet')
else:
    print("Error: 'next_item_prediction' column does not exist in the DataFrame.")

predicted_item_task2['next_item_prediction'][0]

!aicrowd submission create -c task-2-next-product-recommendation-for-underrepresented-languages -f "submission_task2.parquet"

"""# **Rough Approaches and Working**"""

# Initialize GPU
from sklearn.metrics.pairwise import cosine_similarity
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('GPU device not found. Using CPU instead.')
    device_name = '/device:CPU:0'

# Assign the text preprocessing process to a GPU
with tf.device(device_name):
    result_train = np.zeros((1000, 42503))
    from sklearn.metrics.pairwise import cosine_similarity
    for i, batch2 in enumerate(matrix2_batches):
        print(batch2.shape)
    for j, batch1 in enumerate(matrix1_batches):
        # Perform dot product on the current batch pair
        p = -1
        for submatrix2 in batch2:
            print(p)
            p = p+1
            q = -1
            for submatrix1 in batch1:
                q = q+1
                batch_result = np.dot(submatrix2, submatrix1)[0][0]
                norm_sub1 = np.linalg.norm(submatrix1, axis=1)
                norm_sub2 = np.linalg.norm(submatrix2, axis=1)
                similarity_value = batch_result/(norm_sub1[0]*norm_sub2[0])
                result_train[p][q] = similarity_value

def recommend_items(user_id, top_n=100):
  a=user_id - 3272716
  similarity=result_train[a]
  ranked_items = np.argsort(similarity)[::-1]
  recommended_array = ranked_items[:top_n]
  recommended_items=[]
  for x in recommended_array:
    recommended_items.append(products_ES['id'][x])
  return recommended_items

predicted_item=pd.DataFrame(columns=['locale','next_item_purchased'], index=range(1000))
for user_id in range(3272716, 3273716):
  recommended_items = recommend_items(user_id, top_n=100)
  a=user_id - 3272716
  predicted_item['locale'][a]='ES'
  predicted_item['next_item_purchased'][a]=recommended_items

predicted_item

MR=0
n=0
for i in range(3272716, 3273716):
  n=n+1
  user_id=i
  p=i-3272716
  x=train_sessions_ES['next_item'][i]
  recommended_items=predicted_item['next_item_purchased'][p]
  a=1
  for j in recommended_items:
      if x==j:
          break
      else:
          a+=1
  if a<101:
      pred_score=1/a
  else:
      pred_score=0
  MR=(MR*(n-1)+pred_score)/n
  print(MR)

def encode_textual_user_features(textual_features):
  # Get the vector representation for each textual feature
  encoded_features = {}
  for feature in textual_features:
    if feature!='price':
      encoded_feature=[]
      for sentences in textual_features[feature]:
        for sentence in sentences:
          for word in sentence.split():
            if word in model.wv:
              encoded_feature.append(model.wv[word])
            else:
              print(feature,word)
      encoded_features[feature] = np.mean(encoded_feature, axis=0)
  return encoded_features

from sklearn.metrics.pairwise import cosine_similarity
def recommend_items(user_id, item_profiles, top_n=100):
  purchases = user_purchase_history[user_id]

  # Collect the user profile features from the purchases
  user_features = ['price', 'title', 'brand', 'color', 'model', 'material', 'desc']
  profile = {feature: [] for feature in user_features}

  for purchase in purchases:
    for feature in user_features:
      profile[feature].append(products_ES.loc[products_ES['id'] == purchase, feature].tolist())

  encoded_user_profile=encode_textual_user_features(profile)
  # Calculate the similarity between user profile and item profiles
  similarities = {}
  for item_id in encoded_item_profiles:
    for feature in encoded_item_profiles[item_id]:
      similarity = 0.0
      item_feature_values = encoded_item_profiles[item_id][feature]
      user_feature_values = encoded_user_profile[feature]
      feature_similarity = cosine_similarity([user_feature_values], [item_feature_values])
      similarity += feature_similarity

      similarities[item_id] = similarity

  # Rank items based on similarity
  print(similarities)
  ranked_items = sorted(similarities, key=similarities.get, reverse=True)

  # Select top-N items as recommendations
  recommended_items = [item_id for item_id in ranked_items[:top_n]]

  return recommended_items

# Initialize GPU
from sklearn.metrics.pairwise import cosine_similarity
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('GPU device not found. Using CPU instead.')
    device_name = '/device:CPU:0'

# Assign the text preprocessing process to a GPU
with tf.device(device_name):
  # Example usage
  user_id = 3272720
  recommended_items = recommend_items(user_id, item_profiles, top_n=100)
  #print("Recommended items for user", user_id, ":", recommended_items)
  print(1)

result_train[4]

print("Recommended items for user", user_id, ":", recommended_items)

train_sessions_ES['next_item'][3272720]

predicted_item['next_item_purchased'][4]



test_sessions_ES

test_sessions_ES['prev_items']=test_sessions_ES['prev_items'].apply(preprocess)

test_sessions_ES

user_purchase_history_test = test_sessions_ES.groupby('user_id')['prev_items'].apply(list)

user_purchase_history_test=user_purchase_history_test.apply(lambda x: x[0] if isinstance(x, list) else x)

user_purchase_history_test

# Initialize GPU
from sklearn.metrics.pairwise import cosine_similarity
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('GPU device not found. Using CPU instead.')
    device_name = '/device:CPU:0'

# Assign the text preprocessing process to a GPU
with tf.device(device_name):
    user_profiles_ES = {}
    for i in range(0, 8177):
         purchases = user_purchase_history_test[i]
         user_features = ['price', 'title', 'brand',
                           'color', 'model', 'material', 'desc']
         profile = {feature: [] for feature in user_features}
         for purchase in purchases:
              for feature in user_features:
                    profile[feature].append(
                   products_ES.loc[products_ES['id'] == purchase, feature].tolist())
         user_profiles_ES[i] = profile

user_profiles_ES

def encode_textual_user_features_test(textual_features):
  # Get the vector representation for each textual feature
  encoded_user_feature={}
  for user_test_id in textual_features:
    encoded_features = {}
    for feature in textual_features[user_test_id]:
      if feature!='price':
        encoded_feature=[]
        for sentences in textual_features[user_test_id][feature]:
          for sentence in sentences:
            for word in sentence.split():
              if word in model.wv:
                encoded_feature.append(model.wv[word])
              else:
                print(feature,word)
        encoded_features[feature] = np.mean(encoded_feature, axis=0)
    encoded_user_feature[user_test_id]=encoded_features
  return encoded_user_feature

encoded_user_profile_ES=encode_textual_user_features_test(user_profiles_ES)

c=[]
for user_id in encoded_user_profile_ES:
  b=[]
  for feature in encoded_user_profile_ES[user_id]:
    a=encoded_user_profile_ES[user_id][feature]
    b.append(a)
  c.append(b)
encoded_user_profile_ES_dict=np.array(c)

c=[]
for item_id in encoded_item_profiles:
  b=[]
  for feature in encoded_item_profiles[item_id]:
    a=encoded_item_profiles[item_id][feature]
    b.append(a)
  c.append(b)
encoded_item_profiles_dict=np.array(c)

matrix1=encoded_item_profiles_dict.reshape(42503,100,6)
matrix2=encoded_user_profile_ES_dict
print(matrix1.shape,matrix2.shape)

batch_size = 1000  # Adjust the batch size as per your memory constraints
# Split matrix1 and matrix2 into batches
matrix1_batches = np.array_split(matrix1, len(matrix1) // batch_size)
matrix2_batches = np.array_split(matrix2, len(matrix2) // batch_size)

# Initialize GPU
from sklearn.metrics.pairwise import cosine_similarity
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('GPU device not found. Using CPU instead.')
    device_name = '/device:CPU:0'

# Assign the text preprocessing process to a GPU
with tf.device(device_name):
  result = np.zeros((8177,42503))  # Initialize the result array

  for i, batch2 in enumerate(matrix2_batches):
      print(batch2.shape)
      for j, batch1 in enumerate(matrix1_batches):
          # Perform dot product on the current batch pair
          p=-1
          for submatrix2 in batch2:
               p=p+1
               q=-1
               for submatrix1 in batch1:
                  q=q+1
                  batch_result=np.dot(submatrix2,submatrix1)
                  result[p][q]=batch_result[0][0]

def recommend_items_test(user_id, top_n=100):
  similarity=result[user_id]
  ranked_items = np.argsort(similarity)[::-1]
  recommended_array = ranked_items[:top_n]
  recommended_items=[]
  for x in recommended_array:
    recommended_items.append(products_ES['id'][x])
  return recommended_items

user_id = 0
recommended_items = recommend_items_test(user_id, top_n=100)
print("Recommended items for user", user_id, ":", recommended_items)

predicted_item_task2=pd.DataFrame(columns=['locale','next_item_purchased'], index=range(34690))
for user_id in range(8177):
  recommended_items = recommend_items_test(user_id, top_n=100)
  predicted_item_task2['locale'][user_id]='ES'
  predicted_item_task2['next_item_purchased'][user_id]=recommended_items

predicted_item_task2.head(8180)

predicted_item_task2.to_parquet(f'submission_{Task}.parquet', engine='pyarrow')

!aicrowd submission create -c task-2-next-product-recommendation-for-underrepresented-languages -f "submission_task2.parquet"

# import numpy as np
# from sklearn.metrics.pairwise import cosine_similarity

# def recommend_items_test(user_id, top_n=100):
#   user_profile = np.array(encoded_user_profile_ES_dict[user_id])
#   item_profiles = np.array(list(encoded_item_profiles_dict.values()))

#   similarity=[]
#   a=0
#   for submatrix in item_profiles:
#     similarity.append(cosine_similarity(user_profile, submatrix)[0][0])
#   #for i in similarity:
#    # i=i+similarity_matrix[a][user_id]
#     #a=a+1
#   # Rank items based on similarity
#   ranked_items = np.argsort(similarity)[::-1]
#   # Select top-N items as recommendations
#   recommended_array = ranked_items[:top_n]
#   recommended_items=[]
#   for x in recommended_array:
#     recommended_items.append(products_ES['id'][x])
#   return recommended_items

# predicted_item_task2=pd.DataFrame(columns=['id','next_item_purchased'], index=range(34690))
# for user_id in range(8177):
#   recommended_items = recommend_items_test(user_id, top_n=100)
#   predicted_item_task2['id'][user_id]=user_id
#   predicted_item_task2['next_item_purchased'][user_id]=recommended_items

# predicted_item_task2.head(249)

# user_id = 2
# recommended_items = recommend_items_test(user_id, top_n=100)
# print("Recommended items for user", user_id, ":", recommended_items)
# print(1)

# import numpy as np
# from scipy.spatial.distance import euclidean

# # Convert user prices to NumPy array and flatten
# for user_id in range(8177):
#   user_profiles_ES[user_id]['price']=np.array(np.array(user_profiles_ES[user_id]['price']).flatten().tolist())

# # Get item prices
# item_prices = np.array(products_ES['price'])

# # Preallocate similarity matrix
# similarity_matrix = np.zeros((42503, 8177))

# # Calculate similarity for each item and user
# for item_id in range(42503):
#   for user_id in range(8177):
#     item_price = item_prices[item_id]
#     user_prices=user_profiles_ES[user_id]['price']
#     item_prices_array = np.full_like(user_prices, item_price)
#     distances = euclidean(user_prices, item_prices_array)
#     similarities = 1 / (1 + distances)
#     similarity_matrix[item_id][user_id] = similarities

# print("Similarity Matrix:", similarity_matrix)

# MR=0
# n=0
# for i in range(3272716, 3361763):
#   n=n+1
#   user_id=i
#   recommended_items = recommend_items(user_id, item_profiles, top_n=100)
#   x=train_sessions_ES['next_item'][i]
#   a=1
#   for j in recommended_items:
#       if x==j:
#           break
#       else:
#           a+=1
#   if a<101:
#       pred_score=1/a
#   else:
#       pred_score=0
#   MR=(MR*(n-1)+pred_score)/n
#   print(MR)

